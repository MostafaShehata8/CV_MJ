{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GPqGxhERqaYk",
        "R4KwYTkIpnlK",
        "ABdqqkW0pu3S",
        "xNfUS_IJpFfY",
        "AVlP16FRp-FB",
        "Kbov3AEsqH9m"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPTQQHGCGN/sqe0AA9o5F/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostafaShehata8/CV_MJ/blob/main/Vision_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **install dependencies**"
      ],
      "metadata": {
        "id": "GPqGxhERqaYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets\n",
        "!pip install opencv-python-headless\n",
        "!pip install scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUHVBEr0WF8j",
        "outputId": "49bce6c3-1889-45ee-fe75-bca54a7f2cf3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.26.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.25.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (17.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets) (0.16)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.66.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import exposure\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "-1EcdsX0fg7M"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "R4KwYTkIpnlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Caltech-101 dataset\n",
        "dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)\n",
        "\n",
        "# Get the training and test datasets\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n"
      ],
      "metadata": {
        "id": "BBwjFBXXc5nP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hog Feature**"
      ],
      "metadata": {
        "id": "ABdqqkW0pu3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import hog\n",
        "\n",
        "def extract_hog_features(image):\n",
        "    \"\"\"\n",
        "    Extracts HOG features from the input image.\n",
        "    \"\"\"\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute the HOG features\n",
        "    fd, hog_image = hog(gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "\n",
        "    # Return both the HOG features (fd) and the HOG image (hog_image)\n",
        "    return fd, hog_image\n",
        "\n"
      ],
      "metadata": {
        "id": "iuZlqRsnfiRW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import exposure\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_hog(image):\n",
        "    \"\"\"\n",
        "    Visualizes HOG features extracted from the input image.\n",
        "    \"\"\"\n",
        "    # Extract HOG features\n",
        "    fd, hog_image = extract_hog_features(image)\n",
        "\n",
        "    # Rescale the intensity of the HOG image for better visibility\n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "    # Plot the HOG image\n",
        "    plt.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
        "    plt.title(\"HOG Features\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S5_6gm_Yn7NI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_at_index(dataset, index):\n",
        "    \"\"\"\n",
        "    Get an image from the dataset at a specific index.\n",
        "    \"\"\"\n",
        "    image, label = list(dataset)[index]\n",
        "    return np.array(image)\n",
        "\n",
        "# Pick the 20th image (change the index as needed)\n",
        "example_image = get_image_at_index(train_dataset, 50)\n",
        "plt.title(\"Original Image\")\n",
        "plt.imshow(example_image)\n",
        "plt.show()\n",
        "\n",
        "# Visualize the HOG features for the selected image\n",
        "visualize_hog(example_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "4lvJBQkZoN_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Color Histogram**\n"
      ],
      "metadata": {
        "id": "xNfUS_IJpFfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_color_histogram(image):\n",
        "    # Convert image to RGB if it's not\n",
        "    if len(image.shape) == 2:  # If grayscale, convert to BGR\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Calculate histograms for each color channel (B, G, R)\n",
        "    hist_b = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
        "    hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])\n",
        "    hist_r = cv2.calcHist([image], [2], None, [256], [0, 256])\n",
        "\n",
        "    # Normalize the histograms\n",
        "    hist_b /= hist_b.sum()\n",
        "    hist_g /= hist_g.sum()\n",
        "    hist_r /= hist_r.sum()\n",
        "\n",
        "    # Concatenate the histograms of the three channels\n",
        "    hist = np.concatenate([hist_b, hist_g, hist_r])\n",
        "\n",
        "    return hist\n"
      ],
      "metadata": {
        "id": "MgzYRsnYiGHo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **local binary patterns histogram**"
      ],
      "metadata": {
        "id": "AVlP16FRp-FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_lbp_histogram(image, radius=1, n_points=8):\n",
        "    # Convert image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply LBP\n",
        "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
        "\n",
        "    # Calculate histogram of LBP\n",
        "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "\n",
        "    # Normalize the histogram\n",
        "    lbp_hist = lbp_hist.astype('float')\n",
        "    lbp_hist /= (lbp_hist.sum() + 1e-6)\n",
        "\n",
        "    return lbp_hist\n"
      ],
      "metadata": {
        "id": "znjbNbbxiTPm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Combine features**"
      ],
      "metadata": {
        "id": "Kbov3AEsqH9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image):\n",
        "    # Resize image to a standard size\n",
        "    image = cv2.resize(image, (224, 224))\n",
        "\n",
        "    # Extract HOG features\n",
        "    hog_features = extract_hog_features(image)\n",
        "\n",
        "    # Extract color histogram\n",
        "    color_hist = extract_color_histogram(image)\n",
        "\n",
        "    # Extract LBP histogram\n",
        "    lbp_hist = extract_lbp_histogram(image)\n",
        "\n",
        "    # Combine all features into a single vector\n",
        "    features = np.concatenate([hog_features.flatten(), color_hist.flatten(), lbp_hist.flatten()])\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "siJckCFAiWFj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(dataset):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for image, label in dataset:\n",
        "        image = np.array(image)\n",
        "        feature_vector = extract_features(image)\n",
        "\n",
        "        features.append(feature_vector)\n",
        "        labels.append(label.numpy())\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Process the training set\n",
        "train_features, train_labels = process_dataset(train_dataset)\n",
        "\n",
        "print(\"Features shape:\", train_features.shape)\n",
        "print(\"Labels shape:\", train_labels.shape)\n"
      ],
      "metadata": {
        "id": "DvsXq7g_i-8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}